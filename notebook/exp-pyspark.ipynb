{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12404078,"sourceType":"datasetVersion","datasetId":7822434}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T15:58:55.741924Z","iopub.execute_input":"2025-07-12T15:58:55.742300Z","iopub.status.idle":"2025-07-12T15:58:56.788793Z","shell.execute_reply.started":"2025-07-12T15:58:55.742269Z","shell.execute_reply":"2025-07-12T15:58:56.787931Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"spark = SparkSession.builder.getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T15:58:56.790369Z","iopub.execute_input":"2025-07-12T15:58:56.790749Z","iopub.status.idle":"2025-07-12T15:59:04.006132Z","shell.execute_reply.started":"2025-07-12T15:58:56.790726Z","shell.execute_reply":"2025-07-12T15:59:04.005107Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/07/12 15:59:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df = spark.read.parquet(\n    \"/kaggle/input/fda-cleaned-data/part-00000-f7897fa7-5c35-48b0-808e-96b66af99d3b-c000.snappy.parquet\",\n    \"/kaggle/input/fda-cleaned-data/part-00001-f7897fa7-5c35-48b0-808e-96b66af99d3b-c000.snappy.parquet\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T15:59:04.007439Z","iopub.execute_input":"2025-07-12T15:59:04.007828Z","iopub.status.idle":"2025-07-12T15:59:09.086925Z","shell.execute_reply.started":"2025-07-12T15:59:04.007789Z","shell.execute_reply":"2025-07-12T15:59:09.085933Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- Set top-K values ---\ntop_k_reactions = 100\ntop_k_drugs = 50\n\n# --- Get top K reactions ---\ntop_reactions = df.groupBy(\"reaction\").count().orderBy(\"count\", ascending=False).limit(top_k_reactions)\ntop_reaction_values = [row[\"reaction\"] for row in top_reactions.collect()]\n\n# Replace rare reactions with \"other\"\ndf = df.withColumn(\n    \"reaction_filtered\",\n    when(col(\"reaction\").isin(top_reaction_values), col(\"reaction\")).otherwise(lit(\"other\"))\n)\n\n# --- Get top K drugs ---\ntop_drugs = df.groupBy(\"drug\").count().orderBy(\"count\", ascending=False).limit(top_k_drugs)\ntop_drug_values = [row[\"drug\"] for row in top_drugs.collect()]\n\n# Replace rare drugs with \"other\"\ndf = df.withColumn(\n    \"drug_filtered\",\n    when(col(\"drug\").isin(top_drug_values), col(\"drug\")).otherwise(lit(\"other\"))\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T15:59:09.088018Z","iopub.execute_input":"2025-07-12T15:59:09.088442Z","iopub.status.idle":"2025-07-12T15:59:19.394982Z","shell.execute_reply.started":"2025-07-12T15:59:09.088412Z","shell.execute_reply":"2025-07-12T15:59:19.394105Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df.toParquet()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:05:40.737729Z","iopub.execute_input":"2025-07-12T16:05:40.738106Z","iopub.status.idle":"2025-07-12T16:05:41.078214Z","shell.execute_reply.started":"2025-07-12T16:05:40.738076Z","shell.execute_reply":"2025-07-12T16:05:41.077176Z"}},"outputs":[{"name":"stdout","text":"+----+---+-------+--------+----------------+--------------------+---------+-----------------+-------------+\n| age|sex|country|reaction|reaction_outcome|                drug|age_group|reaction_filtered|drug_filtered|\n+----+---+-------+--------+----------------+--------------------+---------+-----------------+-------------+\n|77.0|2.0|     US|Delirium|             1.0|            NUPLAZID|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|             ASPIRIN|    71-80|            other|      ASPIRIN|\n|77.0|2.0|     US|Delirium|             1.0|        ATORVASTATIN|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|            BACLOFEN|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|          BUDESONIDE|    71-80|            other|   BUDESONIDE|\n|77.0|2.0|     US|Delirium|             1.0|  CARBIDOPA\\LEVODOPA|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|          FINGOLIMOD|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|          FLUOXETINE|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|         FLUTICASONE|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|          FORMOTEROL|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|          GABAPENTIN|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|         LATANOPROST|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|       LEVOTHYROXINE|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|           MELATONIN|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|           ALBUTEROL|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|          VITAMIN D3|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|          ROPINIROLE|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|           BIOFREEZE|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|CRANBERRY [ASCORB...|    71-80|            other|        other|\n|77.0|2.0|     US|Delirium|             1.0|            DEPAKOTE|    71-80|            other|        other|\n+----+---+-------+--------+----------------+--------------------+---------+-----------------+-------------+\nonly showing top 20 rows\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"categorical_cols = [\"country\", \"reaction_filtered\", \"drug_filtered\", \"age_group\"]\n\n\n# Create StringIndexers for each\nindexers = [\n    StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"keep\")\n    for col in categorical_cols\n]\n\n# Fit and transform using a pipeline\npipeline = Pipeline(stages=indexers)\ndf_indexed = pipeline.fit(df).transform(df_indexed if 'df_indexed' in locals() else df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:05:12.664692Z","iopub.execute_input":"2025-07-12T16:05:12.665073Z","iopub.status.idle":"2025-07-12T16:05:35.714907Z","shell.execute_reply.started":"2025-07-12T16:05:12.665032Z","shell.execute_reply":"2025-07-12T16:05:35.713728Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"df_indexed = df_indexed.withColumnRenamed(\"reaction_outcome\", \"label\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T21:34:29.186155Z","iopub.execute_input":"2025-07-11T21:34:29.186623Z","iopub.status.idle":"2025-07-11T21:34:29.210052Z","shell.execute_reply.started":"2025-07-11T21:34:29.186590Z","shell.execute_reply":"2025-07-11T21:34:29.208937Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\n\nfeature_cols = [\"age\", \"sex\"] + [f\"{col}_indexed\" for col in categorical_cols]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n\nml_df = assembler.transform(df_indexed).select(\"features\", \"label\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T23:59:43.303643Z","iopub.execute_input":"2025-07-07T23:59:43.303989Z","iopub.status.idle":"2025-07-07T23:59:43.443084Z","shell.execute_reply.started":"2025-07-07T23:59:43.303960Z","shell.execute_reply":"2025-07-07T23:59:43.442018Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"ml_df.show(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T23:59:43.444533Z","iopub.execute_input":"2025-07-07T23:59:43.444824Z","iopub.status.idle":"2025-07-07T23:59:44.194811Z","shell.execute_reply.started":"2025-07-07T23:59:43.444801Z","shell.execute_reply":"2025-07-07T23:59:44.193851Z"}},"outputs":[{"name":"stdout","text":"+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n|[77.0,2.0,1.0,0.0...|  1.0|\n+--------------------+-----+\nonly showing top 10 rows\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"ml_df.groupBy(\"label\").count().orderBy(\"count\", ascending=False).show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T23:59:44.196433Z","iopub.execute_input":"2025-07-07T23:59:44.197346Z","iopub.status.idle":"2025-07-07T23:59:45.986900Z","shell.execute_reply.started":"2025-07-07T23:59:44.197313Z","shell.execute_reply":"2025-07-07T23:59:45.986142Z"}},"outputs":[{"name":"stderr","text":"[Stage 20:===========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-----+--------+\n|label|   count|\n+-----+--------+\n|  5.0|12891444|\n|  6.0| 7636419|\n|  3.0| 5799012|\n|  1.0| 2013425|\n|  2.0|  758002|\n|  4.0|   52808|\n+-----+--------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\nprint(f\"Train count: {train_df.count()}, Test count: {test_df.count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T23:59:45.988714Z","iopub.execute_input":"2025-07-07T23:59:45.990413Z","iopub.status.idle":"2025-07-08T00:02:03.412154Z","shell.execute_reply.started":"2025-07-07T23:59:45.990378Z","shell.execute_reply":"2025-07-08T00:02:03.411300Z"}},"outputs":[{"name":"stderr","text":"[Stage 26:===========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"Train count: 23324234, Test count: 5826876\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50, maxBins=200)\nrf_model = rf.fit(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:02:03.413476Z","iopub.execute_input":"2025-07-08T00:02:03.414214Z","iopub.status.idle":"2025-07-08T00:24:49.163574Z","shell.execute_reply.started":"2025-07-08T00:02:03.414182Z","shell.execute_reply":"2025-07-08T00:24:49.162445Z"}},"outputs":[{"name":"stderr","text":"25/07/08 00:06:20 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 101.2 MiB so far)\n25/07/08 00:06:20 WARN BlockManager: Persisting block rdd_104_1 to disk instead.\n25/07/08 00:06:34 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 151.9 MiB so far)\n25/07/08 00:06:34 WARN BlockManager: Persisting block rdd_104_0 to disk instead.\n25/07/08 00:07:54 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 360.0 MiB so far)\n25/07/08 00:08:33 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 360.0 MiB so far)\n25/07/08 00:09:55 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 151.9 MiB so far)\n25/07/08 00:09:55 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 228.0 MiB so far)\n25/07/08 00:12:15 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 151.9 MiB so far)\n25/07/08 00:12:15 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 228.0 MiB so far)\n25/07/08 00:15:34 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 151.9 MiB so far)\n25/07/08 00:15:35 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 228.0 MiB so far)\n25/07/08 00:19:43 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 151.9 MiB so far)\n25/07/08 00:19:44 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 228.0 MiB so far)\n                                                                                \r","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"predictions = rf_model.transform(test_df)\npredictions.select(\"prediction\", \"label\", \"features\").show(5, truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:24:49.165114Z","iopub.execute_input":"2025-07-08T00:24:49.165457Z","iopub.status.idle":"2025-07-08T00:25:37.143852Z","shell.execute_reply.started":"2025-07-08T00:24:49.165425Z","shell.execute_reply":"2025-07-08T00:25:37.142588Z"}},"outputs":[{"name":"stderr","text":"[Stage 46:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+----------+-----+--------------+\n|prediction|label|features      |\n+----------+-----+--------------+\n|5.0       |6.0  |(6,[0],[41.0])|\n|5.0       |6.0  |(6,[0],[41.0])|\n|5.0       |6.0  |(6,[0],[41.0])|\n|5.0       |6.0  |(6,[0],[41.0])|\n|5.0       |6.0  |(6,[0],[41.0])|\n+----------+-----+--------------+\nonly showing top 5 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n)\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:25:37.149808Z","iopub.execute_input":"2025-07-08T00:25:37.151669Z","iopub.status.idle":"2025-07-08T00:27:55.265355Z","shell.execute_reply.started":"2025-07-08T00:25:37.151615Z","shell.execute_reply":"2025-07-08T00:27:55.263639Z"}},"outputs":[{"name":"stderr","text":"[Stage 47:===========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.6347\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"f1_evaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n)\nf1 = f1_evaluator.evaluate(predictions)\nprint(f\"F1 Score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:27:55.266501Z","iopub.execute_input":"2025-07-08T00:27:55.266820Z","iopub.status.idle":"2025-07-08T00:30:09.966929Z","shell.execute_reply.started":"2025-07-08T00:27:55.266788Z","shell.execute_reply":"2025-07-08T00:30:09.965997Z"}},"outputs":[{"name":"stderr","text":"[Stage 49:===========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.5914\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"predictions.groupBy(\"prediction\").count().orderBy(\"count\", ascending=False).show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T00:30:09.968412Z","iopub.execute_input":"2025-07-08T00:30:09.969044Z","iopub.status.idle":"2025-07-08T00:32:19.235604Z","shell.execute_reply.started":"2025-07-08T00:30:09.969015Z","shell.execute_reply":"2025-07-08T00:32:19.232829Z"}},"outputs":[{"name":"stderr","text":"25/07/08 00:30:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/07/08 00:30:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n[Stage 51:===========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+----------+-------+\n|prediction|  count|\n+----------+-------+\n|       5.0|3406463|\n|       6.0|1257206|\n|       3.0|1160637|\n|       1.0|   2570|\n+----------+-------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from pyspark.ml.classification import (\n    LogisticRegression,\n    DecisionTreeClassifier,\n    RandomForestClassifier,\n    GBTClassifier,\n    NaiveBayes\n)\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# ðŸš€ Define your models\nmodels = {\n    \"LogisticRegression\": LogisticRegression(labelCol=\"label\", featuresCol=\"features\"),\n    # \"DecisionTree\": DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\"),\n    # \"RandomForest\": RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\"),\n    \"GBT\": GBTClassifier(labelCol=\"label\", featuresCol=\"features\"),\n    \"NaiveBayes\": NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n}\n\n# ðŸŽ¯ Evaluator\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n\n# ðŸ”„ Train + evaluate each model\nfor name, model in models.items():\n    fitted_model = model.fit(train_df)\n    predictions = fitted_model.transform(test_df)\n    acc = evaluator.evaluate(predictions)\n    print(f\"{name} Accuracy: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T01:00:06.732047Z","iopub.execute_input":"2025-07-08T01:00:06.732766Z","iopub.status.idle":"2025-07-08T01:16:41.988288Z","shell.execute_reply.started":"2025-07-08T01:00:06.732729Z","shell.execute_reply":"2025-07-08T01:16:41.986127Z"}},"outputs":[{"name":"stderr","text":"25/07/08 01:02:40 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:02:40 WARN BlockManager: Persisting block rdd_288_1 to disk instead.\n25/07/08 01:02:53 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 113.0 MiB so far)\n25/07/08 01:02:53 WARN BlockManager: Persisting block rdd_288_0 to disk instead.\n25/07/08 01:03:10 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 272.8 MiB so far)\n25/07/08 01:03:36 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 419.0 MiB so far)\n25/07/08 01:03:44 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:03:44 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:03:54 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:03:54 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:05 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:05 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:15 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:15 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:27 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:28 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:39 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:39 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:49 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:49 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:59 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:04:59 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:12 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:12 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:22 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:22 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:32 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:32 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:43 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:43 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:54 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:05:54 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:04 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:04 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:15 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 113.0 MiB so far)\n25/07/08 01:06:15 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 272.8 MiB so far)\n25/07/08 01:06:26 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:27 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:38 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:38 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:49 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:06:49 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:00 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:00 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:10 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:10 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:24 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:24 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:36 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:37 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:49 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:07:49 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:00 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:00 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:11 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:11 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:22 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:22 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:34 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:34 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:47 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:47 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:57 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:08:57 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:10 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:10 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:23 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:23 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:36 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:36 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:48 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:09:48 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:01 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:01 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:13 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:13 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:25 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:25 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:38 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:38 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:51 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:10:51 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:02 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:02 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:13 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:13 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:24 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:24 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:34 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:34 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:45 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:45 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:58 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:11:58 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:07 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:07 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:19 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:19 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:29 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:30 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:40 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:40 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:52 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:12:52 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:02 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:02 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:12 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:12 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:22 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:22 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:33 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:33 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:45 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:45 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:56 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:13:56 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:14:08 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:14:08 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n25/07/08 01:14:18 WARN MemoryStore: Not enough space to cache rdd_288_1 in memory! (computed 177.0 MiB so far)\n25/07/08 01:14:19 WARN MemoryStore: Not enough space to cache rdd_288_0 in memory! (computed 177.0 MiB so far)\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"LogisticRegression Accuracy: 0.5250\n","output_type":"stream"},{"name":"stderr","text":"25/07/08 01:16:41 ERROR Executor: Exception in task 0.0 in stage 182.0 (TID 624)\njava.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n25/07/08 01:16:41 WARN TaskSetManager: Lost task 0.0 in stage 182.0 (TID 624) (fed674ed51ff executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n25/07/08 01:16:41 ERROR TaskSetManager: Task 0 in stage 182.0 failed 1 times; aborting job\n25/07/08 01:16:41 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 182.0 failed 1 times, most recent failure: Lost task 0.0 in stage 182.0 (TID 624) (fed674ed51ff executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/646430746.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# ðŸ”„ Train + evaluate each model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfitted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitted_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o921.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 182.0 failed 1 times, most recent failure: Lost task 0.0 in stage 182.0 (TID 624) (fed674ed51ff executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"],"ename":"Py4JJavaError","evalue":"An error occurred while calling o921.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 182.0 failed 1 times, most recent failure: Lost task 0.0 in stage 182.0 (TID 624) (fed674ed51ff executor driver): java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:201)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.RuntimeException: Labels MUST be in {0, 1}, but got 6.0\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}