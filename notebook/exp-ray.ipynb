{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12450707,"sourceType":"datasetVersion","datasetId":7854062}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"git+https://github.com/ray-project/xgboost_ray.git#egg=xgboost_ray\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nimport ray\nfrom xgboost_ray import RayDMatrix, RayParams, train, predict\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nfrom ray.tune.integration.xgboost import TuneReportCheckpointCallback as XGBoostCallback\nfrom ray.tune.integration.lightgbm import TuneReportCheckpointCallback as LightGBMCallback\n\nray.init(ignore_reinit_error=True)\n\n\n# ============================================\n# üß† XGBOOST SECTION (Raw categorical features)\n# ============================================\n\ndef load_fda_train_data():\n    df = pd.read_parquet(\"/kaggle/input/fd-data-revamp-2/fd_train_df.parquet\")\n    for col in [\"country\", \"reaction\", \"drug\", \"age_group\"]:\n        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n    X = df.drop(columns=[\"reaction_outcome\"])\n    y = df[\"reaction_outcome\"].astype(int)\n    # y = label_encoder.fit_transform(y)\n    return train_test_split(X, y, test_size=0.2, random_state=42, stratify=df[\"reaction_outcome\"])\n\ndef load_fda_val_data():\n    df = pd.read_parquet(\"/kaggle/input/fd-data-revamp-2/fd_val_df.parquet\")\n    df = df.reset_index(drop=True)\n    for col in [\"country\", \"reaction\", \"drug\", \"age_group\"]:\n        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n    X = df.drop(columns=[\"reaction_outcome\"])\n    y = df[\"reaction_outcome\"].astype(int)\n    # y = label_encoder.fit_transform(y)\n    return X, y\n\ndef train_xgb(config):\n    train_x, test_x, train_y, test_y = load_fda_train_data()\n    dtrain = xgb.DMatrix(train_x, label=train_y)\n    dtest = xgb.DMatrix(test_x, label=test_y)\n    xgb.train(\n        config,\n        dtrain,\n        evals=[(dtest, \"eval\")],\n        verbose_eval=False,\n        callbacks=[XGBoostCallback(frequency=1)],\n    )\n\ndef run_xgb_tuning():\n    search_space = {\n        \"objective\": \"multi:softprob\",\n        \"eval_metric\": [\"mlogloss\"],\n        \"num_class\": 6,\n        \"max_depth\": tune.randint(3, 8),\n        \"min_child_weight\": tune.choice([1, 2, 3]),\n        \"subsample\": tune.uniform(0.5, 1.0),\n        \"eta\": tune.loguniform(1e-4, 1e-1),\n        \"tree_method\": \"hist\",\n        \"device\": \"cuda\", # Explicitly use GPU\n    }\n    # search_space = {\n    #         \"objective\": \"multi:softprob\",\n    #         \"eval_metric\": [\"mlogloss\"],\n    #         \"num_class\": 6,\n    #         \"max_depth\": tune.randint(3, 15),\n    #         \"min_child_weight\": tune.randint(1, 10),\n    #         \"subsample\": tune.uniform(0.5, 1.0),\n    #         \"colsample_bytree\": tune.uniform(0.5, 1.0),\n    #         \"eta\": tune.loguniform(1e-4, 1e-1),\n    #         \"gamma\": tune.uniform(0, 5),\n    #         \"tree_method\": \"hist\",\n    #         \"device\": \"cuda\",      \n    # }\n    tuner = tune.Tuner(\n        # train_xgb,\n        tune.with_resources(train_xgb, {\"cpu\": 4, \"gpu\": 1}),\n        # resources_per_trial=ray_params.get_tune_resources(),\n        tune_config=tune.TuneConfig(\n            metric=\"eval-mlogloss\", mode=\"min\",\n            scheduler=ASHAScheduler(\n                max_t=100,  # Maximum number of iterations\n                grace_period=10,  # Minimum number of iterations\n                reduction_factor=2  # Reduction factor for stopping trials\n            ),\n            num_samples=5\n            # max_concurrent_trials=2\n        ),\n        param_space=search_space,\n    )\n    results = tuner.fit()\n    best_result = results.get_best_result()\n    best_model = XGBoostCallback.get_model(best_result.checkpoint)\n    val_x, val_y = load_fda_val_data()\n    y_pred = np.argmax(best_model.predict(xgb.DMatrix(val_x)), axis=1)\n    print(\"‚úÖ XGBoost Best Params:\", best_result.config)\n    print(\"üéØ XGBoost Accuracy:\", accuracy_score(val_y, y_pred))\n    print(classification_report(val_y, y_pred))\n    best_model.save_model(\"fda_best_xgboost_model.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:59:28.639499Z","iopub.execute_input":"2025-07-12T16:59:28.639794Z","iopub.status.idle":"2025-07-12T16:59:34.802458Z","shell.execute_reply.started":"2025-07-12T16:59:28.639769Z","shell.execute_reply":"2025-07-12T16:59:34.801522Z"}},"outputs":[{"name":"stderr","text":"2025-07-12 16:59:33,174\tINFO worker.py:1917 -- Started a local Ray instance.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(\"üöÄ Running XGBoost Tuning...\")\nrun_xgb_tuning()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T16:59:34.803766Z","iopub.execute_input":"2025-07-12T16:59:34.805229Z","iopub.status.idle":"2025-07-12T17:03:45.534560Z","shell.execute_reply.started":"2025-07-12T16:59:34.805200Z","shell.execute_reply":"2025-07-12T17:03:45.533826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div class=\"tuneStatus\">\n  <div style=\"display: flex;flex-direction: row\">\n    <div style=\"display: flex;flex-direction: column;\">\n      <h3>Tune Status</h3>\n      <table>\n<tbody>\n<tr><td>Current time:</td><td>2025-07-12 17:03:28</td></tr>\n<tr><td>Running for: </td><td>00:03:54.03        </td></tr>\n<tr><td>Memory:      </td><td>3.8/31.4 GiB       </td></tr>\n</tbody>\n</table>\n    </div>\n    <div class=\"vDivider\"></div>\n    <div class=\"systemInfo\">\n      <h3>System Info</h3>\n      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: -1.616782322185865<br>Logical resource usage: 4.0/4 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:P100)\n    </div>\n    \n  </div>\n  <div class=\"hDivider\"></div>\n  <div class=\"trialStatus\">\n    <h3>Trial Status</h3>\n    <table>\n<thead>\n<tr><th>Trial name           </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">       eta</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  subsample</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  eval-mlogloss</th></tr>\n</thead>\n<tbody>\n<tr><td>train_xgb_96564_00000</td><td>TERMINATED</td><td>172.19.2.2:3273</td><td style=\"text-align: right;\">0.04132   </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   0.737775</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         42.8676</td><td style=\"text-align: right;\">        1.46931</td></tr>\n<tr><td>train_xgb_96564_00001</td><td>TERMINATED</td><td>172.19.2.2:3378</td><td style=\"text-align: right;\">0.0178838 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   0.535441</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         42.5368</td><td style=\"text-align: right;\">        1.61678</td></tr>\n<tr><td>train_xgb_96564_00002</td><td>TERMINATED</td><td>172.19.2.2:3473</td><td style=\"text-align: right;\">0.00161641</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">   0.928591</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         42.7193</td><td style=\"text-align: right;\">        1.77349</td></tr>\n<tr><td>train_xgb_96564_00003</td><td>TERMINATED</td><td>172.19.2.2:3565</td><td style=\"text-align: right;\">0.00307973</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   0.504781</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         42.8188</td><td style=\"text-align: right;\">        1.75907</td></tr>\n<tr><td>train_xgb_96564_00004</td><td>TERMINATED</td><td>172.19.2.2:3658</td><td style=\"text-align: right;\">0.0949067 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">   0.849916</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         43.563 </td><td style=\"text-align: right;\">        1.17442</td></tr>\n</tbody>\n</table>\n  </div>\n</div>\n<style>\n.tuneStatus {\n  color: var(--jp-ui-font-color1);\n}\n.tuneStatus .systemInfo {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus td {\n  white-space: nowrap;\n}\n.tuneStatus .trialStatus {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus h3 {\n  font-weight: bold;\n}\n.tuneStatus .hDivider {\n  border-bottom-width: var(--jp-border-width);\n  border-bottom-color: var(--jp-border-color0);\n  border-bottom-style: solid;\n}\n.tuneStatus .vDivider {\n  border-left-width: var(--jp-border-width);\n  border-left-color: var(--jp-border-color0);\n  border-left-style: solid;\n  margin: 0.5em 1em 0.5em 1em;\n}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"\u001b[36m(train_xgb pid=3273)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3273)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3273)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/xgboost/_xgboost_utils.py:170: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n\u001b[36m(train_xgb pid=3273)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3273)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n\u001b[36m(train_xgb pid=3273)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n\u001b[36m(train_xgb pid=3273)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3273)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3273)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000000)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000001)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000002)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000003)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000004)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000005)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000006)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000007)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000008)\n\u001b[36m(train_xgb pid=3273)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00000_0_eta=0.0413,max_depth=3,min_child_weight=1,subsample=0.7378_2025-07-12_16-59-34/checkpoint_000009)\n\u001b[36m(train_xgb pid=3378)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3378)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3378)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/xgboost/_xgboost_utils.py:170: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n\u001b[36m(train_xgb pid=3378)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3378)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n\u001b[36m(train_xgb pid=3378)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n\u001b[36m(train_xgb pid=3378)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3378)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3378)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000000)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000001)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000002)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000003)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000004)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000005)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000006)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000007)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000008)\n\u001b[36m(train_xgb pid=3378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00001_1_eta=0.0179,max_depth=4,min_child_weight=2,subsample=0.5354_2025-07-12_16-59-34/checkpoint_000009)\n\u001b[36m(train_xgb pid=3473)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3473)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3473)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/xgboost/_xgboost_utils.py:170: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n\u001b[36m(train_xgb pid=3473)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3473)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n\u001b[36m(train_xgb pid=3473)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n\u001b[36m(train_xgb pid=3473)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3473)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3473)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000000)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000001)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000002)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000003)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000004)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000005)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000006)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000007)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000008)\n\u001b[36m(train_xgb pid=3473)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00002_2_eta=0.0016,max_depth=4,min_child_weight=2,subsample=0.9286_2025-07-12_16-59-34/checkpoint_000009)\n\u001b[36m(train_xgb pid=3565)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3565)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3565)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/xgboost/_xgboost_utils.py:170: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n\u001b[36m(train_xgb pid=3565)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3565)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n\u001b[36m(train_xgb pid=3565)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n\u001b[36m(train_xgb pid=3565)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3565)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3565)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000000)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000001)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000002)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000003)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000004)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000005)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000006)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000007)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000008)\n\u001b[36m(train_xgb pid=3565)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00003_3_eta=0.0031,max_depth=3,min_child_weight=1,subsample=0.5048_2025-07-12_16-59-34/checkpoint_000009)\n\u001b[36m(train_xgb pid=3658)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3658)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3658)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/xgboost/_xgboost_utils.py:170: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n\u001b[36m(train_xgb pid=3658)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3658)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n\u001b[36m(train_xgb pid=3658)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n\u001b[36m(train_xgb pid=3658)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3658)\u001b[0m /usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n\u001b[36m(train_xgb pid=3658)\u001b[0m   _log_deprecation_warning(\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000000)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000001)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000002)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000003)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000004)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000005)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000006)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000007)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000008)\n\u001b[36m(train_xgb pid=3658)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_xgb_2025-07-12_16-59-34/train_xgb_96564_00004_4_eta=0.0949,max_depth=6,min_child_weight=1,subsample=0.8499_2025-07-12_16-59-34/checkpoint_000009)\n2025-07-12 17:03:28,866\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_xgb_2025-07-12_16-59-34' in 0.0060s.\n2025-07-12 17:03:28,872\tINFO tune.py:1041 -- Total run time: 234.05 seconds (234.03 seconds for the tuning loop).\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ XGBoost Best Params: {'objective': 'multi:softprob', 'eval_metric': ['mlogloss'], 'num_class': 6, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.8499162438945151, 'eta': 0.09490665818154173, 'tree_method': 'hist', 'device': 'cuda'}\nüéØ XGBoost Accuracy: 0.36397258972299856\n              precision    recall  f1-score   support\n\n           0       0.05      0.12      0.07    402685\n           1       0.51      0.03      0.06    151600\n           2       0.68      0.38      0.49   1159802\n           3       0.06      0.01      0.02     10562\n           4       0.71      0.17      0.28   2578289\n           5       0.34      0.77      0.47   1527284\n\n    accuracy                           0.36   5830222\n   macro avg       0.39      0.25      0.23   5830222\nweighted avg       0.55      0.36      0.35   5830222\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install lazypredict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom lazypredict.Supervised import LazyClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load your prepared data\ntrain_df = pd.read_parquet(\"/kaggle/input/fd-data-revamp-2/fd_train_df.parquet\")\n# val_df = pd.read_parquet(\"../data/fd_val_df.parquet\")\n\n# Combine train and validation for LazyPredict (you can split again)\n# Or use your existing split if you prefer\nX = train_df.drop(columns=['reaction_outcome'])  # Features\ny = train_df['reaction_outcome']  # Target\n\n# Split data (unless you want to use your predefined val_df)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=train_df[\"reaction_outcome\"], random_state=42\n)\n\n# Initialize LazyClassifier\nclf = LazyClassifier(\n    verbose=0,\n    ignore_warnings=True,\n    custom_metric=None,\n    predictions=False,\n    random_state=42,\n    classifiers='all'  # or specify particular ones\n)\n\n# Fit and evaluate models\nmodels, predictions = clf.fit(X_train, X_test, y_train, y_test)\n\n# Display results\nprint(models)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-12T16:59:12.989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom lazypredict.Supervised import LazyClassifier\n\n# Prepare features with VectorAssembler\nfeature_cols = [c for c in df.columns if c != \"reaction_outcome\"]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to pandas\npandas_df = df.toPandas()\nX = pandas_df[feature_cols]\ny = pandas_df[\"reaction_outcome\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use LazyPredict\nclf = LazyClassifier()\nmodels, predictions = clf.fit(X_train, X_test, y_train, y_test)\n\n# Show model performance\nprint(models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom xgboost_ray import RayDMatrix, RayParams, train, predict\nfrom ray import tune\nimport ray\n\n# Initialize Ray with 2 GPUs\nray.init(num_cpus=4, num_gpus=2, ignore_reinit_error=True)\n\n# ============================================\n# üß† DATA LOADING & PROCESSING\n# ============================================\n\ndef load_fda_train_data():\n    df = pd.read_parquet(\"/kaggle/input/fda-data-revamp/fd_train_df.parquet\")\n    for col in [\"country\", \"reaction\", \"drug\", \"age_group\"]:\n        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n    X = df.drop(columns=[\"reaction_outcome\"])\n    y = df[\"reaction_outcome\"].astype(int)\n    y = LabelEncoder().fit_transform(y)\n    return train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef load_fda_val_data():\n    df = pd.read_parquet(\"/kaggle/input/fda-data-revamp/fd_val_df.parquet\")\n    for col in [\"country\", \"reaction\", \"drug\", \"age_group\"]:\n        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n    X = df.drop(columns=[\"reaction_outcome\"])\n    y = df[\"reaction_outcome\"].astype(int)\n    y = LabelEncoder().fit_transform(y)\n    return X, y\n\n# ============================================\n# üéØ RAY TUNE TRIAL FUNCTION\n# ============================================\n\n# num_actors = 2  # 2 actors for 2 GPUs\nray_params = RayParams(\n    num_actors=2,\n    cpus_per_actor=2,\n    gpus_per_actor=1\n)\n\n\ndef train_model(config):\n    train_x, test_x, train_y, test_y = load_fda_train_data()\n\n    dtrain = RayDMatrix(train_x, train_y)\n    dtest = RayDMatrix(test_x, test_y)\n\n    evals_result = {}\n    booster = train(\n        params=config,\n        dtrain=dtrain,\n        evals=[(dtest, \"eval\")],\n        evals_result=evals_result,\n        ray_params=ray_params,\n        verbose_eval=False,\n    )\n\n    # Save best model\n    booster.save_model(\"model.xgb\")\n\n    # Compute accuracy\n    y_pred_prob = predict(booster, dtest, ray_params=ray_params)\n    y_pred = np.argmax(y_pred_prob, axis=1)\n    acc = accuracy_score(test_y, y_pred)\n\n    tune.report(eval_error=1 - acc)  # or use mlogloss\n\n# ============================================\n# üîç HYPERPARAMETER SEARCH SPACE\n# ============================================\n\nsearch_space = {\n    \"objective\": \"multi:softprob\",\n    \"num_class\": 6,\n    \"eval_metric\": \"mlogloss\",\n    \"tree_method\": \"gpu_hist\",\n    \"eta\": tune.loguniform(1e-4, 1e-1),\n    \"subsample\": tune.uniform(0.5, 1.0),\n    \"colsample_bytree\": tune.uniform(0.5, 1.0),\n    \"gamma\": tune.uniform(0, 5),\n    \"max_depth\": tune.randint(3, 15),\n    \"min_child_weight\": tune.randint(1, 10),\n}\n\n# ============================================\n# üß™ RUN TUNING JOB\n# ============================================\ntuner = tune.Tuner(\n    # tune.with_resources(train_model, {\"cpu\": 2, \"gpu\": 1} ),\n    tune.with_resources(train_model, ray_params.get_tune_resources()),\n    tune_config=tune.TuneConfig(\n        metric=\"eval-mlogloss\",\n        mode=\"min\",\n        num_samples=10,\n    ),\n    run_config=tune.RunConfig(name=\"xgboost_ray_multi_gpu\"),\n    param_space=search_space,\n)\n\nresults = tuner.fit()\nprint(\"‚úÖ Best Config:\", results.get_best_result().config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss\n\nimport ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\n# We won't use the direct TuneReportCheckpointCallback for K-fold average reporting.\n# Instead, we'll manually report the average metric.\n# from ray.tune.integration.xgboost import TuneReportCheckpointCallback as XGBoostCallback\n\nray.init(ignore_reinit_error=True)\n\n# ============================================\n# üß† Data Loading Functions\n# ============================================\n\ndef load_fda_full_train_data():\n    \"\"\"Loads the full training data for K-fold cross-validation.\"\"\"\n    df = pd.read_parquet(\"/kaggle/input/fda-data-revamp/fd_train_df.parquet\")\n    for col in [\"country\", \"reaction\", \"drug\", \"age_group\"]:\n        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n    X = df.drop(columns=[\"reaction_outcome\"])\n    y = df[\"reaction_outcome\"].astype(int)\n    y = LabelEncoder().fit_transform(y)  # maps to [0, 1, 2, ..., n_classes-1]\n    return X, y\n\ndef load_fda_val_data():\n    \"\"\"Loads the separate validation data for final evaluation.\"\"\"\n    df = pd.read_parquet(\"/kaggle/input/fda-data-revamp/fd_val_df.parquet\")\n    for col in [\"country\", \"reaction\", \"drug\", \"age_group\"]:\n        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n    X = df.drop(columns=[\"reaction_outcome\"])\n    y = df[\"reaction_outcome\"].astype(int)\n    y = LabelEncoder().fit_transform(y)\n    return X, y\n\n# ============================================\n# üß† XGBOOST SECTION (Raw categorical features)\n# ============================================\n\ndef train_xgb_kfold(config):\n    X, y = load_fda_full_train_data()\n    num_classes = len(np.unique(y))\n    config[\"num_class\"] = num_classes\n    \n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    mlogloss_scores = []\n    accuracy_scores = []\n\n    for fold_idx, (train_index, val_index) in enumerate(kf.split(X, y)):\n        train_x, val_x = X.iloc[train_index], X.iloc[val_index]\n        train_y, val_y = y[train_index], y[val_index]\n\n        dtrain = xgb.DMatrix(train_x, label=train_y)\n        dval = xgb.DMatrix(val_x, label=val_y)\n\n        model = xgb.train(\n            config,\n            dtrain,\n            evals=[(dval, \"validation\")],\n            verbose_eval=False,\n        )\n\n        val_preds_proba = model.predict(dval)\n        val_preds_labels = np.argmax(val_preds_proba, axis=1)\n\n        mlogloss_scores.append(log_loss(val_y, val_preds_proba))\n        accuracy_scores.append(accuracy_score(val_y, val_preds_labels))\n\n    avg_mlogloss = np.mean(mlogloss_scores)\n    avg_accuracy = np.mean(accuracy_scores)\n    \n    # Correct way to report metrics:\n    tune.report(avg_mlogloss=avg_mlogloss, avg_accuracy=avg_accuracy)\n\ndef run_xgb_tuning_kfold():\n    search_space = {\n        \"objective\": \"multi:softprob\",\n        \"eval_metric\": [\"mlogloss\"], # Note: 'eval_metric' for XGBoost internal monitoring, not directly for Tune\n        \"num_class\": 6,\n        \"max_depth\": tune.randint(3, 8),\n        \"min_child_weight\": tune.choice([1, 2, 3]),\n        \"subsample\": tune.uniform(0.5, 1.0),\n        \"eta\": tune.loguniform(1e-4, 1e-1),\n        \"tree_method\": \"hist\",\n        \"device\": \"cuda\", # Explicitly use GPU\n    }\n    tuner = tune.Tuner(\n        tune.with_resources(train_xgb_kfold, resources={\"cpu\": 4, \"gpu\": 1}), # Allocate GPU for each trial\n        tune_config=tune.TuneConfig(\n            metric=\"avg_mlogloss\", mode=\"min\", # Tune based on averaged mlogloss\n            scheduler=ASHAScheduler(), num_samples=10), # Adjust num_samples based on resources\n        param_space=search_space,\n    )\n    results = tuner.fit()\n    best_result = results.get_best_result()\n\n    print(\"‚úÖ XGBoost Best Params (from K-fold tuning):\", best_result.config)\n    print(\"üéØ XGBoost Average mlogloss (on K-fold validation sets):\", best_result.metrics.get(\"avg_mlogloss\"))\n    print(\"üéØ XGBoost Average Accuracy (on K-fold validation sets):\", best_result.metrics.get(\"avg_accuracy\"))\n\n\n    # --- Final Evaluation on the Separate Validation Set (fd_val_df.parquet) ---\n    print(\"\\n--- Final XGBoost Model Training and Evaluation ---\")\n    val_x, val_y = load_fda_val_data() # Load the truly unseen validation data\n\n    # Train a final model with the best parameters on the full training data\n    X_full_train, y_full_train = load_fda_full_train_data()\n    d_full_train = xgb.DMatrix(X_full_train, label=y_full_train)\n\n    final_xgb_model = xgb.train(\n        best_result.config,\n        d_full_train,\n        num_boost_round=1000, # Example: Train for more rounds\n        callbacks=[xgb.callback.EarlyStopping(rounds=50, metric_name=\"mlogloss\", data_name=\"train\", maximize=False, save_best=True)] # Simple early stopping if desired\n    )\n\n    y_pred_val = np.argmax(final_xgb_model.predict(xgb.DMatrix(val_x)), axis=1)\n    print(\"üéØ XGBoost Accuracy on Final Unseen Validation Set:\", accuracy_score(val_y, y_pred_val))\n    print(f\"üéØ XGBoost Log Loss on Final Unseen Validation Set: {log_loss(val_y, final_xgb_model.predict(xgb.DMatrix(val_x))):.4f}\")\n    print(\"\\nClassification Report (XGBoost on Final Unseen Validation Set):\\n\", classification_report(val_y, y_pred_val))\n    print(\"\\nConfusion Matrix (XGBoost on Final Unseen Validation Set):\\n\", confusion_matrix(val_y, y_pred_val))\n    final_xgb_model.save_model(\"fda_best_xgboost_final_model.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting XGBoost K-fold Tuning...\")\nrun_xgb_tuning_kfold()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ray.shutdown()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}